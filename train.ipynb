{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd05f1bf990545e0d616dda92c7aa1749f4ea584ed2622d2307d8ecb02d5b9184c3",
   "display_name": "Python 3.8.8 64-bit ('TDT4265': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "5f1bf990545e0d616dda92c7aa1749f4ea584ed2622d2307d8ecb02d5b9184c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torch import nn\n",
    "\n",
    "from DatasetLoader import DatasetLoader\n",
    "from Unet2D import Unet2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n",
    "    start = time.time()\n",
    "    model.cuda()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "\n",
    "                running_acc  += acc*dataloader.batch_size\n",
    "                running_loss += loss*dataloader.batch_size \n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('-' * 10)\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cuda()).float().mean()\n",
    "\n",
    "def batch_to_img(xb, idx):\n",
    "    img = np.array(xb[idx,0:3])\n",
    "    return img.transpose((1,2,0))\n",
    "\n",
    "def predb_to_mask(predb, idx):\n",
    "    p = torch.functional.F.softmax(predb[idx], 0)\n",
    "    return p.argmax(0).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main ():\n",
    "    #enable if you want to see some plotting\n",
    "    visual_debug = True\n",
    "\n",
    "    #batch size\n",
    "    bs = 12\n",
    "\n",
    "    #epochs\n",
    "    epochs_val = 50\n",
    "\n",
    "    #learning rate\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    #sets the matplotlib display backend (most likely not needed)\n",
    "    mp.use('TkAgg', force=True)\n",
    "\n",
    "    #load the training data\n",
    "    base_path = Path('./home/gkiss/Data/CAMUS_resized')\n",
    "    data = DatasetLoader(base_path/'train_gray', \n",
    "                        base_path/'train_gt')\n",
    "    print(len(data))\n",
    "\n",
    "    #split the training dataset and initialize the data loaders\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(data, (300, 150))\n",
    "    train_data = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    valid_data = DataLoader(valid_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].imshow(data.open_as_array(150))\n",
    "        ax[1].imshow(data.open_mask(150))\n",
    "        plt.show()\n",
    "\n",
    "    xb, yb = next(iter(train_data))\n",
    "    print (xb.shape, yb.shape)\n",
    "\n",
    "    # build the Unet2D with one channel as input and 2 channels as output\n",
    "    unet = Unet2D(1,2)\n",
    "\n",
    "    #loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(unet.parameters(), lr=learn_rate)\n",
    "\n",
    "    #do some training\n",
    "    train_loss, valid_loss = train(unet, train_data, valid_data, loss_fn, opt, acc_metric, epochs=epochs_val)\n",
    "\n",
    "    #plot training and validation losses\n",
    "    if visual_debug:\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(train_loss, label='Train loss')\n",
    "        plt.plot(valid_loss, label='Valid loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    #predict on the next train batch (is this fair?)\n",
    "    xb, yb = next(iter(train_data))\n",
    "    with torch.no_grad():\n",
    "        predb = unet(xb.cuda())\n",
    "\n",
    "    #show the predicted segmentations\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(bs,3, figsize=(15,bs*5))\n",
    "        for i in range(bs):\n",
    "            ax[i,0].imshow(batch_to_img(xb,i))\n",
    "            ax[i,1].imshow(yb[i])\n",
    "            ax[i,2].imshow(predb_to_mask(predb, i))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "450\n",
      "torch.Size([12, 1, 384, 384]) torch.Size([12, 384, 384])\n",
      "Epoch 0/49\n",
      "----------\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.5206 Acc: 0.902328610420227\n",
      "----------\n",
      "Epoch 0/49\n",
      "----------\n",
      "valid Loss: 0.3820 Acc: 0.9709836840629578\n",
      "----------\n",
      "Epoch 1/49\n",
      "----------\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.2258 Acc: 0.9317498803138733\n",
      "----------\n",
      "Epoch 1/49\n",
      "----------\n",
      "valid Loss: 0.1807 Acc: 0.9701107740402222\n",
      "----------\n",
      "Epoch 2/49\n",
      "----------\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.1574 Acc: 0.9317498207092285\n",
      "----------\n",
      "Epoch 2/49\n",
      "----------\n",
      "valid Loss: 0.1979 Acc: 0.9701799750328064\n",
      "----------\n",
      "Epoch 3/49\n",
      "----------\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.1378 Acc: 0.9317498207092285\n",
      "----------\n",
      "Epoch 3/49\n",
      "----------\n",
      "valid Loss: 0.1608 Acc: 0.9710049629211426\n",
      "----------\n",
      "Epoch 4/49\n",
      "----------\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.1291 Acc: 0.9317498207092285\n",
      "----------\n",
      "Epoch 4/49\n",
      "----------\n",
      "valid Loss: 0.1298 Acc: 0.97055983543396\n",
      "----------\n",
      "Epoch 5/49\n",
      "----------\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.931749701499939\n",
      "----------\n",
      "Epoch 5/49\n",
      "----------\n",
      "valid Loss: 0.1250 Acc: 0.9711522459983826\n",
      "----------\n",
      "Epoch 6/49\n",
      "----------\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 0.1176 Acc: 0.9317498207092285\n",
      "----------\n",
      "Epoch 6/49\n",
      "----------\n",
      "valid Loss: 0.1298 Acc: 0.9713578820228577\n",
      "----------\n",
      "Epoch 7/49\n",
      "----------\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 0.1112 Acc: 0.9317498803138733\n",
      "----------\n",
      "Epoch 7/49\n",
      "----------\n",
      "valid Loss: 0.1166 Acc: 0.9706511497497559\n",
      "----------\n",
      "Epoch 8/49\n",
      "----------\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 0.1052 Acc: 0.9317498207092285\n",
      "----------\n",
      "Epoch 8/49\n",
      "----------\n",
      "valid Loss: 0.1052 Acc: 0.9714130759239197\n",
      "----------\n",
      "Epoch 9/49\n",
      "----------\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 0.1007 Acc: 0.931749701499939\n",
      "----------\n",
      "Epoch 9/49\n",
      "----------\n",
      "valid Loss: 0.1178 Acc: 0.9706161618232727\n",
      "----------\n",
      "Epoch 10/49\n",
      "----------\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 0.0969 Acc: 0.931749701499939\n",
      "----------\n",
      "Epoch 10/49\n",
      "----------\n",
      "valid Loss: 0.0992 Acc: 0.9707521796226501\n",
      "----------\n",
      "Epoch 11/49\n",
      "----------\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.9317498207092285\n",
      "----------\n",
      "Epoch 11/49\n",
      "----------\n",
      "valid Loss: 0.1083 Acc: 0.9706255197525024\n",
      "----------\n",
      "Epoch 12/49\n",
      "----------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-9e2a0111c323>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#do some training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#plot training and validation losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-5e6fe0233d75>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# iterate over data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}