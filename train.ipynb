{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torch import nn\n",
    "\n",
    "from DatasetLoader import DatasetLoader\n",
    "from Unet2D import Unet2D\n",
    "\n",
    "from decouple import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BASE_PATH=config('IMAGE_BASE_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, dice_fn, epochs=1):\n",
    "    start = time.time()\n",
    "    model.cuda()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            running_dice = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "                dice_score = dice_fn(outputs, y)\n",
    "\n",
    "                running_acc  += acc*dataloader.batch_size\n",
    "                running_loss += loss*dataloader.batch_size\n",
    "                running_dice += dice_score*dataloader.batch_size \n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  Dice: {}  AllocMem (Mb): {}'.format(step, loss, acc, dice_score, torch.cuda.memory_allocated()/1024/1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "            epoch_dice = running_dice / len(dataloader.dataset)\n",
    "\n",
    "            print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('{} Loss: {:.4f} Acc: {} Dice: {}'.format(phase, epoch_loss, epoch_acc, epoch_dice))\n",
    "            print('-' * 10)\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cuda()).float().mean()\n",
    "\n",
    "def dice_score(predb, yb):\n",
    "    predflat = predb.argmax(dim=1).view(-1)\n",
    "    yflat = yb.view(-1)\n",
    "    intersection = (predflat * yflat).sum()\n",
    "    return (2 * intersection) / (predflat.sum() + yflat.sum())\n",
    "\n",
    "def batch_to_img(xb, idx):\n",
    "    img = np.array(xb[idx,0:3])\n",
    "    return img.transpose((1,2,0))\n",
    "\n",
    "def predb_to_mask(predb, idx):\n",
    "    p = torch.functional.F.softmax(predb[idx], 0)\n",
    "    return p.argmax(0).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main ():\n",
    "    #enable if you want to see some plotting\n",
    "    visual_debug = True\n",
    "\n",
    "    #batch size\n",
    "    bs = 12\n",
    "\n",
    "    #epochs\n",
    "    epochs_val = 50\n",
    "\n",
    "    #learning rate\n",
    "    learn_rate = 0.01\n",
    "\n",
    "    #sets the matplotlib display backend (most likely not needed)\n",
    "    mp.use('TkAgg', force=True)\n",
    "\n",
    "    #load the training data git \n",
    "    base_path = Path(DATA_BASE_PATH)\n",
    "    data = DatasetLoader(base_path/'train_gray', \n",
    "                        base_path/'train_gt')\n",
    "    print(len(data))\n",
    "\n",
    "    #split the training dataset and initialize the data loaders\n",
    "    train_dataset, valid_dataset = torch.utils.data.random_split(data, (300, 150))\n",
    "    train_data = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    valid_data = DataLoader(valid_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        ax[0].imshow(data.open_as_array(150))\n",
    "        ax[1].imshow(data.open_mask(150))\n",
    "        plt.show()\n",
    "\n",
    "    xb, yb = next(iter(train_data))\n",
    "    print (xb.shape, yb.shape)\n",
    "\n",
    "    # build the Unet2D with one channel as input and 2 channels as output\n",
    "    unet = Unet2D(1,2)\n",
    "\n",
    "    #loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(unet.parameters(), lr=learn_rate)\n",
    "\n",
    "    #do some training\n",
    "    train_loss, valid_loss = train(unet, train_data, valid_data, loss_fn, opt, acc_metric, dice_score, epochs=epochs_val)\n",
    "\n",
    "    #plot training and validation losses\n",
    "    if visual_debug:\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(train_loss, label='Train loss')\n",
    "        plt.plot(valid_loss, label='Valid loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    #predict on the next train batch (is this fair?)\n",
    "    xb, yb = next(iter(train_data))\n",
    "    with torch.no_grad():\n",
    "        predb = unet(xb.cuda())\n",
    "\n",
    "    #show the predicted segmentations\n",
    "    if visual_debug:\n",
    "        fig, ax = plt.subplots(bs,3, figsize=(15,bs*5))\n",
    "        for i in range(bs):\n",
    "            ax[i,0].imshow(batch_to_img(xb,i))\n",
    "            ax[i,1].imshow(yb[i])\n",
    "            ax[i,2].imshow(predb_to_mask(predb, i))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "450\n",
      "torch.Size([12, 1, 384, 384]) torch.Size([12, 384, 384])\n",
      "Epoch 0/49\n",
      "----------\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 0.4190 Acc: 0.9243042469024658 Dice: 0.010783287696540356\n",
      "----------\n",
      "Epoch 0/49\n",
      "----------\n",
      "valid Loss: 0.3051 Acc: 0.9718979001045227 Dice: 0.0\n",
      "----------\n",
      "Epoch 1/49\n",
      "----------\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 0.1861 Acc: 0.9314709901809692 Dice: 0.0\n",
      "----------\n",
      "Epoch 1/49\n",
      "----------\n",
      "valid Loss: 0.1625 Acc: 0.9713572263717651 Dice: 0.0\n",
      "----------\n",
      "Epoch 2/49\n",
      "----------\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.931471049785614 Dice: 0.0\n",
      "----------\n",
      "Epoch 2/49\n",
      "----------\n",
      "valid Loss: 0.1307 Acc: 0.9710515737533569 Dice: 0.0\n",
      "----------\n",
      "Epoch 3/49\n",
      "----------\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.1288 Acc: 0.931471049785614 Dice: 0.0\n",
      "----------\n",
      "Epoch 3/49\n",
      "----------\n",
      "valid Loss: 0.1271 Acc: 0.971878707408905 Dice: 0.0\n",
      "----------\n",
      "Epoch 4/49\n",
      "----------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-ddeb7c54d2e7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m#do some training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#plot training and validation losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-2b8671f65021>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, dice_fn, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# iterate over data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd05f1bf990545e0d616dda92c7aa1749f4ea584ed2622d2307d8ecb02d5b9184c3",
   "display_name": "Python 3.8.8 64-bit ('TDT4265': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "5f1bf990545e0d616dda92c7aa1749f4ea584ed2622d2307d8ecb02d5b9184c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}